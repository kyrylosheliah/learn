{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%precision 2\n",
    "import numpy as np\n",
    "\n",
    "class Leaky_ReLU:\n",
    "    @staticmethod\n",
    "    def forward(x):\n",
    "        data = [[max(0.05*value,value) for value in row] for row in x]\n",
    "        return np.array(data, dtype=float)\n",
    "    @staticmethod\n",
    "    def backward(x):\n",
    "        data = [[1 if value>0 else 0.05 for value in row] for row in x]\n",
    "        return np.array(data, dtype=float)\n",
    "\n",
    "class ReLU:\n",
    "    @staticmethod\n",
    "    def forward(x):\n",
    "        return np.maximum(0, x)\n",
    "    @staticmethod\n",
    "    def backward(x):\n",
    "        return 1. * (x > 0)\n",
    "    \n",
    "class tanh:\n",
    "    @staticmethod\n",
    "    def forward(x):\n",
    "        return np.tanh(x)\n",
    "    @staticmethod\n",
    "    def backward(x):\n",
    "        return 1-np.tanh(x)**2\n",
    "\n",
    "class sigmoid:\n",
    "    @staticmethod\n",
    "    def forward(x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    @staticmethod\n",
    "    def backward(x):\n",
    "        fx = sigmoid.forward(x)\n",
    "        return fx*(1-fx)\n",
    "\n",
    "# loss function and its derivative\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean(np.power(y_pred-y_true, 2))\n",
    "def mse_prime(y_true, y_pred):\n",
    "    return 2*np.mean(y_pred-y_true)\n",
    "\n",
    "# Base class\n",
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "    def forward_propagation(self, input):\n",
    "        raise NotImplementedError\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class ActivationLayer(Layer):\n",
    "    def __init__(self, function):\n",
    "        self.activation = function.forward\n",
    "        self.activation_prime = function.backward\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = self.activation(self.input)\n",
    "        return self.output\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        return self.activation_prime(self.input) * output_error\n",
    "\n",
    "class FCLayer(Layer):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.rand(input_size, output_size)\n",
    "        self.bias = np.random.rand(1, output_size)\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = np.dot(self.input, self.weights) + self.bias\n",
    "        return self.output\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        input_error = np.dot(output_error, self.weights.T)\n",
    "        weights_error = np.dot(self.input.T, output_error)\n",
    "        self.weights -= learning_rate * weights_error\n",
    "        self.bias -= learning_rate * output_error\n",
    "        return input_error\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, loss, loss_prime):\n",
    "        self.layers = []\n",
    "        self.loss = loss\n",
    "        self.loss_prime = loss_prime\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    def use(self, loss, loss_prime):\n",
    "        self.loss = loss\n",
    "        self.loss_prime = loss_prime\n",
    "    def predict(self, input_data):\n",
    "        sample_count = input_data.shape[0]\n",
    "        result = []\n",
    "        for i in range(sample_count):\n",
    "            output = input_data[i]\n",
    "            for layer in self.layers:\n",
    "                output = layer.forward_propagation(output)\n",
    "            result.append(output)\n",
    "        return result\n",
    "    def fit(self, x_train, y_train, epochs, learning_rate):\n",
    "        x_train = np.array([[elem] for elem in x_train])\n",
    "        y_train = np.array([[elem] for elem in y_train])\n",
    "        err = 0\n",
    "        for i in range(epochs):\n",
    "            err = 0\n",
    "            for j in range(x_train.shape[0]):\n",
    "                output = x_train[j]\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward_propagation(output)\n",
    "                err += self.loss(y_train[j], output)\n",
    "                error = self.loss_prime(y_train[j], output)\n",
    "                for layer in reversed(self.layers):\n",
    "                    error = layer.backward_propagation(error, learning_rate)\n",
    "        print(f'fit result of {epochs} epochs, error={err}, learning_rate={learning_rate}')\n",
    "        return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize_data(data, prefix=''):\n",
    "    minimum = np.min(data)\n",
    "    maximum = np.max(data)\n",
    "    diff = maximum - minimum\n",
    "    bias = minimum if minimum >= 0 else -minimum\n",
    "    normalized_data = (data-bias)/diff\n",
    "    print(\"{prefix}diff={diff}, {prefix}bias={bias}\")\n",
    "    return normalized_data, diff, bias\n",
    "\n",
    "def training_compare(x_train, y_train, output, x_diff=0, x_bias=0, y_diff=0, y_bias=0):\n",
    "    if x_diff==0 and x_bias==0:\n",
    "        display_x_train = x_train\n",
    "    else:\n",
    "        display_x_train = np.copy(x_train)*x_diff+x_bias\n",
    "    if y_diff==0 and y_bias==0:\n",
    "        display_y_train = y_train\n",
    "        display_output = output\n",
    "    else:\n",
    "        display_y_train = np.copy(y_train)*y_diff+y_bias\n",
    "        display_output = np.copy(output)*y_diff+y_bias\n",
    "    for i in range(display_x_train.shape[0]):\n",
    "        print(f'expected {display_x_train[i]}\\tto {display_y_train[i]}\\tgot {display_output[i]}')\n",
    "\n",
    "def prediction_display(input, output, x_diff=0, x_bias=0, y_diff=0, y_bias=0):\n",
    "    if x_diff==0 and x_bias==0:\n",
    "        display_input = input\n",
    "    else:\n",
    "        display_input = np.copy(input)*x_diff+x_bias\n",
    "    if y_diff==0 and y_bias==0:\n",
    "        display_output = output\n",
    "    else:\n",
    "        display_output = np.copy(output)*y_diff+y_bias\n",
    "    for i in range(display_input.shape[0]):\n",
    "        print(f'inputed { display_input[i]}\\tgot {display_output[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{prefix}diff={diff}, {prefix}bias={bias}\n",
      "{prefix}diff={diff}, {prefix}bias={bias}\n",
      "fit result of 100 epochs, error=0.06423651826423267, learning_rate=0.1\n",
      "fit result of 100 epochs, error=0.00661028648235658, learning_rate=0.08\n",
      "fit result of 100 epochs, error=0.002575767501378715, learning_rate=0.064\n",
      "fit result of 100 epochs, error=0.0015051570722327632, learning_rate=0.0512\n",
      "fit result of 100 epochs, error=0.0010511401627237979, learning_rate=0.04096\n",
      "expected [85.]\tto [28800.]\tgot [[28432.0962456]]\n",
      "expected [88.]\tto [14400.]\tgot [[14965.67130616]]\n",
      "expected [91.]\tto [7200.]\tgot [[7051.79013725]]\n",
      "expected [94.]\tto [3600.]\tgot [[3322.03534113]]\n",
      "expected [97.]\tto [1800.]\tgot [[1645.43925394]]\n",
      "expected [100.]\tto [900.]\tgot [[861.56938852]]\n",
      "expected [103.]\tto [450.]\tgot [[469.75010018]]\n",
      "expected [106.]\tto [225.]\tgot [[259.98880383]]\n",
      "expected [109.]\tto [112.5]\tgot [[140.49331268]]\n",
      "expected [112.]\tto [56.25]\tgot [[68.6105767]]\n",
      "expected [115.]\tto [28.125]\tgot [[23.25249825]]\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([[85],[88],[91],[94],[97],[100],[103],[106],[109],[112],[115]])\n",
    "y_data = np.array([[28800],[14400],[7200],[3600],[1800],[900],[450],[225],[112.5],[56.25],[28.125]])\n",
    "\n",
    "x_train, x_diff, x_bias = normalize_data(x_data, 'x_')\n",
    "y_train, y_diff, y_bias = normalize_data(y_data, 'y_')\n",
    "\n",
    "net = Network(mse, mse_prime)\n",
    "net.add(FCLayer(x_data.shape[1], 5))\n",
    "net.add(ActivationLayer(tanh))\n",
    "net.add(FCLayer(5, 4))\n",
    "net.add(ActivationLayer(tanh))\n",
    "net.add(FCLayer(4, y_data.shape[1]))\n",
    "lr = 0.1\n",
    "for i in range(5):\n",
    "    net.fit(x_train, y_train, epochs=100, learning_rate=lr)\n",
    "    lr /= 1.25\n",
    "\n",
    "out = net.predict(x_train)\n",
    "training_compare(x_train, y_train, out, x_diff, x_bias, y_diff, y_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputed [80.]\tgot [[51411.81198276]]\n",
      "inputed [84.44444444]\tgot [[31322.66311365]]\n",
      "inputed [88.88888889]\tgot [[12042.16691598]]\n",
      "inputed [93.33333333]\tgot [[3913.2576634]]\n",
      "inputed [97.77777778]\tgot [[1384.46150349]]\n",
      "inputed [102.22222222]\tgot [[548.30177942]]\n",
      "inputed [106.66666667]\tgot [[227.63330411]]\n",
      "inputed [111.11111111]\tgot [[86.45227045]]\n",
      "inputed [115.55555556]\tgot [[16.78204563]]\n",
      "inputed [120.]\tgot [[-21.03067827]]\n"
     ]
    }
   ],
   "source": [
    "new_samples = np.array([[(x-x_bias)/x_diff] for x in np.linspace(80, 120, 10)])\n",
    "out = net.predict(new_samples)\n",
    "prediction_display(new_samples, out, x_diff, x_bias, y_diff, y_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{prefix}diff={diff}, {prefix}bias={bias}\n",
      "{prefix}diff={diff}, {prefix}bias={bias}\n",
      "fit result of 100 epochs, error=1.9239013383131458, learning_rate=0.1\n",
      "fit result of 100 epochs, error=0.0044540224088059945, learning_rate=0.08\n",
      "fit result of 100 epochs, error=4.307005520250428e-11, learning_rate=0.064\n",
      "fit result of 100 epochs, error=1.4139742371368468e-16, learning_rate=0.0512\n",
      "fit result of 100 epochs, error=1.9078084133138852e-20, learning_rate=0.04096\n",
      "expected [0. 0. 1.]\tto [0.]\tgot [[1.0487633e-10]]\n",
      "expected [0. 1. 1.]\tto [1.]\tgot [[1.]]\n",
      "expected [1. 0. 1.]\tto [1.]\tgot [[1.]]\n",
      "expected [1. 1. 1.]\tto [0.]\tgot [[-3.13247206e-11]]\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([[0, 0, 1],[0, 1, 1],[1, 0, 1],[1, 1, 1]])\n",
    "y_data = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "x_train, x_diff, x_bias = normalize_data(x_data, 'x_')\n",
    "y_train, y_diff, y_bias = normalize_data(y_data, 'y_')\n",
    "\n",
    "net = Network(mse, mse_prime)\n",
    "net.add(FCLayer(x_data.shape[1], 5))\n",
    "net.add(ActivationLayer(tanh))\n",
    "net.add(FCLayer(5, 4))\n",
    "net.add(ActivationLayer(tanh))\n",
    "net.add(FCLayer(4, y_data.shape[1]))\n",
    "lr = 0.1\n",
    "for i in range(5):\n",
    "    net.fit(x_train, y_train, epochs=100, learning_rate=lr)\n",
    "    lr /= 1.25\n",
    "\n",
    "out = net.predict(x_train)\n",
    "training_compare(x_train, y_train, out, x_diff, x_bias, y_diff, y_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
